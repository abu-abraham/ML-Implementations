{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra\n",
    "\n",
    "Write down the definitions, being careful to specify conditions (if needed) when they exist.\n",
    "\n",
    "1. The eigenvector decomposition of a matrix $C$.\n",
    "2. The solution of a linear system of equations $Ax=b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Eigenvector decomposition\n",
    "    * $C=U\\Lambda U^T$ where the columns of $U$ are the eigenvectors of $C$ and $\\Lambda$ is a diagonal matrix with diagonal elements given by the eigenvalues of $C$. This decomposition is conditioned on $C$ being a (square) symmetric matrix whose entries are real numbers. Eigenvalues $\\mathbf{u}$ and their corresponding eigenvalues $\\lambda$ have the property $C\\mathbf{u}=\\lambda\\mathbf{u}$.\n",
    "    \n",
    "2. Solution of a linear system\n",
    "    * Unique solution: $x=A^{-1}b$ if $A$ is invertible (ie. it is square and of full rank)\n",
    "    * Infinitely many solutions: $x=A^+b+(I-A^+A)w$ where $A^+$ is the Moore-Penrose pseudo-inverse of $A$, $AA^+b=b$ and $w$ is an arbitrary vector \n",
    "    * No solution: $AA^+b\\neq b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product\n",
    "\n",
    "Recall the definition of a dot product between two vectors. Calculate the following:\n",
    "\n",
    "1. The length of a vector\n",
    "    $$x = \n",
    "    \\begin{bmatrix}\n",
    "        0.1\\\\1.2\\\\-3\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "2. The distance between $x$ and\n",
    "    $$y =\n",
    "    \\begin{bmatrix}\n",
    "        -2\\\\0\\\\-0.3\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "3. Are $x$ and $y$ orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{{\\boldsymbol{{#1}}}}$\n",
    "$\\newcommand{\\mat}[1]{{\\boldsymbol{{#1}}}}$\n",
    "$\\newcommand{\\T}{^\\top}$\n",
    "$\\newcommand{\\inv}{^{-1}}$\n",
    "\n",
    "1. The length of a vector is given by the norm $\\|x\\| = \\sqrt{x\\T x}$.\n",
    "$x\\T x = 0.1^2 + 1.2^2 + (-3)^2 = 10.45$ and therefore $\\|x\\| = \\sqrt{10.45} \\approx 3.2326$\n",
    "\n",
    "2. The distance between two vectors is given by $\\|x-y\\|$.\n",
    "$$x-y =\n",
    "\\begin{bmatrix}\n",
    "    2.1\\\\1.2\\\\-2.7\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Therefore the distance between $x$ and $y$ is approximately 3.6249.\n",
    "\n",
    "3. The dot product between $x$ and $y$ is\n",
    "$$x\\T y = 0.1\\times (-2) + 1.2\\times 0 + (-3)\\times(-0.3) = 0.7.$$\n",
    "Since $x$ and $y$ are orthogonal when their dot product is zero, hence\n",
    "they are **not** orthogonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "\n",
    "For a subspace $U = \\left[\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\begin{bmatrix}\n",
    "0\\\\1\\\\2\n",
    "\\end{bmatrix}\n",
    "\\right]\\subset \\RR^3$ and $\\vec x = \\begin{bmatrix}\n",
    "6\\\\0\\\\0\n",
    "\\end{bmatrix}\\in\\RR^3$\n",
    "find the coordinates $\\vec\\lambda$ of $\\vec x$ in terms of the subspace $U$,\n",
    "the projection point $\\pi_U(\\vec x)$ and the projection\n",
    "matrix $\\mat P_\\pi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "First, we see that the generating set of $U$ is a basis (linear\n",
    "independence) and write the basis vectors of $U$ into a matrix $\\mat B\n",
    "= \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  1 & 1 \\\\\n",
    "  1 & 2\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Second, we compute the matrix $\\mat B\\T \\mat B$ and the vector $\\mat\n",
    "B\\T\\vec x$ as\n",
    "\\begin{align}\n",
    "\\mat B\\T\\mat B =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix}\\,\n",
    "\\quad\n",
    "\\mat B\\T\\vec x = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & 2\n",
    "\\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "6\\\\0\\\\0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6\\\\0\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Third, we  solve the normal equation $\\mat B\\T\\mat B\\vec\\lambda = \\mat\n",
    "B\\T\\vec x$ to find $\\vec\\lambda$:\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6\\\\0\n",
    "\\end{bmatrix}\n",
    "\\iff\n",
    "\\vec\\lambda = \\begin{bmatrix}\n",
    "5\\\\-3\n",
    "\\end{bmatrix}\\,.\n",
    "\\end{align}\n",
    "\n",
    "Fourth, the projection $\\pi_U(\\vec x)$ of $\\vec x$ onto $U$,\n",
    "i.e., into the column space of $\\mat B$, can be directly computed via\n",
    "\\begin{align}\n",
    "  \\pi_U(\\vec x) = \\mat\n",
    "  B\\vec\\lambda = \\begin{bmatrix}\n",
    "    5\\\\2\\\\-1\n",
    "\\end{bmatrix}\\,.\n",
    "\\end{align}\n",
    "The corresponding \\emphidx{projection error}\\slash\n",
    "\\emphidx{reconstruction error} is\n",
    "\\begin{align}\\|\\vec x - \\pi_U(\\vec x)\\| = \\left\\|\\begin{bmatrix}1 & -2 &\n",
    "      1\\end{bmatrix}\\T\\right\\| = \\sqrt{6}\\,.\n",
    "\\end{align}\n",
    "\n",
    "Fifth, the projection matrix (for any $\\vec x \\in\\RR^3$) is given by\n",
    "\\begin{align}\n",
    "\\mat P_\\pi = \\mat B(\\mat B\\T\\mat B)\\inv \\mat B\\T = \\frac{1}{6}\n",
    "\\begin{bmatrix}\n",
    "5 & 2 & -1\\\\\n",
    "2 & 2 & 2\\\\\n",
    "-1 & 2 & 5\n",
    "\\end{bmatrix}\\,.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To verify the results, we can (a) check whether the displacement\n",
    "vector $\\pi_U(\\vec x) - \\vec x$ is orthogonal to all basis vectors of $U$,\n",
    "(b) verify that $\\mat P_\\pi = \\mat P_\\pi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment ([do not use pylab](http://carreau.github.io/posts/10-No-PyLab-Thanks.ipynb.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following cost function $ f(X) $ defined\n",
    "over the space of real $ n \\times p $ matrices\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $, and the matrix $ C $ is symmetric, \n",
    "such that $ C = C^T $. The scalar $ \\mu $ is assumed to be larger than the $p$th smallest \n",
    "eigenvalue of $ C $. The matrix $ N $ is diagonal with distinct positive entries\n",
    "on the diagonal.\n",
    "The trace of a square matrix $ A $ is denoted as $ \\trace{A} $, and\n",
    "the Frobenius norm of an arbitrary matrix $ A $ is defined as $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.)\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "3. Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Given $A \\in \\RR^{n \\times p} $, $ p \\le n $, the straightforward implementation of $ \\trace{A^T A} = \\trace{A A^T}$ is first multiplying the matrices\n",
    "and then taking the trace. This is of\n",
    "complexity $ O(p^2 n) $ (or even $ O(p n^2) $ if you are not careful). \n",
    "But this trace can be reformulated as\n",
    "\\begin{equation}\n",
    "  \\trace{A^T A} = \\sum_{i=1}^p (A^T A)_{i,i} \n",
    "                = \\sum_{i=1}^p \\sum_{j=1}^n \\underbrace{(A^T)_{i,j}}_{=A_{j, i}} A_{j,i}\n",
    "                = \\sum_{i=1}^p \\sum_{j=1}^n A_{j,i}^2\n",
    "\\end{equation}\n",
    "So we can implement it with complexity $ O(np)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76405168  0.66279534  0.10604107]\n",
      " [ 0.34885692  0.64995381  0.19702284]\n",
      " [ 0.71261599  0.07005443  0.39382608]\n",
      " [ 0.95618582  0.06696115  0.00667513]\n",
      " [ 0.04406422  0.06224736  0.48055606]]\n",
      "1.85490560502\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def frobenius_norm(A):\n",
    "    \"\"\"Calculate the Frobenius norm of an array or matrix.\n",
    "    A -- array or matrix\n",
    "    \"\"\"\n",
    "    return np.sqrt((np.asarray(A)**2).sum(axis=None))\n",
    "\n",
    "M = np.random.rand(5,3)\n",
    "print(M)\n",
    "print(frobenius_norm(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with matrix argument\n",
    "\n",
    "Implement the cost function defined as $f(X)$ above as a function ```cost_function_for_matrix```\n",
    "in Python.\n",
    "\n",
    "Hint: As good programmers, we do not use global variables in subroutines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79623607147\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def cost_function_for_matrix(X, C, N, mu):\n",
    "    \"\"\"\n",
    "    Calculate the cost function at point X given as a matrix.\n",
    "    X -- current point in matrix form\n",
    "    C -- symmetric matrix\n",
    "    N -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the value of the cost function as a scalar.\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.matrix):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if not isinstance(C, np.matrix):\n",
    "        raise TypeError(\"C is not a matrix\")\n",
    "\n",
    "    if not isinstance(N, np.matrix):\n",
    "        raise TypeError(\"N is not a matrix\")\n",
    "\n",
    "    r1 = 0.5  * np.trace(X.T * C * X * N)\n",
    "    r2 = 0.25 * mu * frobenius_norm(N - X.T * X)**2\n",
    "    return r1 + r2\n",
    "\n",
    "X = np.matrix(np.random.rand(5,3))\n",
    "C = np.random.rand(5,5)\n",
    "C = np.matrix(C+C.T)\n",
    "N = np.matrix(np.diag(np.random.rand(3)))\n",
    "print(cost_function_for_matrix(X,C,N,np.random.rand()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with vector argument\n",
    "\n",
    "Many standard optimisation routines work only with vectors. Fortunately, as vector spaces,\n",
    "the space of matrices $ \\RR^{n \\times p} $ \n",
    "and the space of vectors $ \\RR^{n p} $ are isomorphic. What does this mean?\n",
    "\n",
    "Implement the cost function $ f(X) $ given $ X $ as a vector and call it ```cost_function_for_vector```.\n",
    "Which extra arguments do you need for the cost function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def cost_function_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the cost function at point X given as 1-D array\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the value of the cost function as a scalar\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if X.ndim != 1:\n",
    "        raise ValueError(\"X is not a 1-D vector\")\n",
    "\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    return cost_function_for_matrix(Xmatrix, C, N, mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function \\lstinline$random_matrix_from_eigenvalues$ which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "There are $ n! $ permutations of diagonal elements, but infinitely many matrices\n",
    "with the same set of eigenvalues.\n",
    "\n",
    "In order to construct a random matrix with given eigenvalues $\\lambda_i$, $i=1,\\dots,n$\n",
    "we first create a diagonal matrix $ \\Lambda $ with those eigenvalues on the\n",
    "diagonal. Then we can get another matrix $ A $ with the same eigenvalues as $ \\Lambda $\n",
    "if we apply an arbitrary nonsingular matrix $ B $ to get $ A = B \\Lambda B^{-1} $.\n",
    "(Can you prove that $ A $ and $ \\Lambda $ have the same set of eigenvalues?)\n",
    "\n",
    "If $ B $ is an orthogonal matrix $ Q $, then $ Q^{-1} = Q^T $ and therefore the above\n",
    "formula results in $ A = Q \\Lambda Q^T $ which is much faster to calculate then\n",
    "using the inverse of a matrix.\n",
    "\n",
    "How to get a random orthogonal matrix? We use the QR-decomposition of a matrix which \n",
    "decomposes every arbitrary matrix $ B $ into an orthogonal matrix $ Q $ and an \n",
    "upper-triangular matrix $ R $, $ B = Q R $.\n",
    "\n",
    "The algorithm therefore is\n",
    "1. Choose a random matrix $ B $ by randomly choosing the elements of $ B $.\n",
    "2. Calculate the QR-decomposition $ B = Q R $. (Check that $ B $ is nonsingular\n",
    "      by checking the diagonal of $ R $ has nonzero elements.)\n",
    "3. Calculate $ A =  Q \\Lambda Q^T $, the wanted arbitrary matrix with the\n",
    "      same eigenvalues as $ \\Lambda $.\n",
    "\n",
    "Note: $ A $ and $ \\Lambda $ share the same \\emph{set} of eigenvalues. The order can\n",
    "be arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def random_matrix_from_eigenvalues(E):\n",
    "    \"\"\"Create a square random matrix with a given set of eigenvalues\n",
    "    E -- list of eigenvalues\n",
    "    return the random matrix\n",
    "    \"\"\"\n",
    "    n    = len(E)\n",
    "    # Create a random orthogonal matrix Q via QR decomposition\n",
    "    # of a random matrix A\n",
    "    A    = np.matrix(np.random.rand(n,n))\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    #  similarity transformation with orthogonal\n",
    "    #  matrix leaves eigenvalues intact\n",
    "    return Q * np.diag(E) * Q.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$\n",
    "\n",
    "Use the minimisation functions ```fmin``` or ```fmin_powell``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector```.\n",
    "\n",
    "Hint: Use the argument ```args``` in the minimisation functions  ```fmin``` or ```fmin_powell``` \n",
    "to provide the extra parameters to\n",
    "your cost function ```cost_function_for_vector```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def minimise_f_using_fmin(initialise_proc):\n",
    "    \"\"\"Run minimisation with simplex algorithm.\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 xtol=0.0001,\n",
    "                                 ftol=0.0001,\n",
    "                                 maxiter=None,\n",
    "                                 maxfun=None,\n",
    "                                 full_output = 0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_powell(initialise_proc):\n",
    "    \"\"\"Run minimisation with Powell algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_powell(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 xtol=0.0001,\n",
    "                                 ftol=0.0001,\n",
    "                                 maxiter=None,\n",
    "                                 maxfun=None,\n",
    "                                 full_output = 0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None,\n",
    "                                 direc=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of $f(X)$\n",
    "\n",
    "Calculate the gradient for the cost function $f(X)$ given the\n",
    "inner product on the space of real matrices $ n \\times p $ is defined as\n",
    "\\begin{equation}\n",
    "  \\inner{A}{B} = \\trace{A^T B}\n",
    "\\end{equation}\n",
    "\n",
    "Implement a function ```gradient_for_vector``` which takes $ X $ as a vector, and\n",
    "returns the gradient of $ f(X) $ as a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The definition of the directional derivative is in the slides.\n",
    "A straightforward calculation using the definition gives\n",
    "the directional derivative of the cost function as\n",
    "\n",
    "\\begin{align*}\n",
    "  \\DD f(X) (\\xi) = & \\phantom{+} \\frac{1}{2} \\trace{\\xi^T C X N} \\\\\n",
    "                   & + \\frac{1}{2} \\trace{X^T C \\xi N} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(- \\xi^T X)(N - X^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(- X^T \\xi)(N - X^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(N - X^T X)(- \\xi^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(N - X^T X)(- X^T \\xi)} .               \n",
    "\\end{align*}\n",
    "\n",
    "Note, the shortcut was to replace each occurrence of the variable $ X $\n",
    "in the function $ f(X) $ once with $ \\xi $ and then add together \n",
    "all those expressions. Reason: The directional derivative gives the\n",
    "linear approximation of the infinitesimal change of the function $ f(X) $\n",
    "at $ X $ in direction $ \\xi $. Therefore it must be linear in $ \\xi $.\n",
    "\n",
    "The above expression can be simplified by using that for any matrices\n",
    "$ A, B $,\n",
    "\n",
    "\\begin{align*}\n",
    " \\trace{A^T} & = \\trace{A}, \\\\\n",
    " \\trace{A B} & = \\trace{B A}\n",
    "\\end{align*}\n",
    " \n",
    "From this we get therefore the simplified form\n",
    "\\begin{align*}\n",
    "  \\DD f(X) (\\xi) & = \\trace{\\xi^T C X N} - \\mu \\trace{\\xi^T X(N - X^T X)} \\\\\n",
    "                 & = \\trace{\\xi^T \\left(C X N - \\mu X(N - X^T X) \\right)}\n",
    "\\end{align*}\n",
    "\n",
    "For the given inner product $ \\inner{A}{B} = \\trace{A^T B} $, the gradient\n",
    "will therefore be\n",
    "\n",
    "\\begin{equation}\n",
    "   \\grad f(X) = C X N - \\mu X(N - X^T X)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def gradient_for_matrix(X, C, N, mu):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point in matrix form\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the gradient of the cost function as matrix\n",
    "    \"\"\"\n",
    "    gradient = C * X * N - mu * X * (N - X.T * X)\n",
    "    return gradient\n",
    "\n",
    "def gradient_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the gradient of the cost function as 1-D array\n",
    "    \"\"\"\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    gradient =  gradient_for_matrix(Xmatrix, C, N, mu)\n",
    "    return np.asarray(gradient).reshape((n*p,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$ using the gradient\n",
    "\n",
    "Use the minimisation functions ```fmin_cg``` or ```fmin_bfgs``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector``` utilising the gradient ```gradient_for_vector```.\n",
    "\n",
    "Compare the speed of convergence to the minimisation with ```fmin``` or ```fmin_powell```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def normalize_columns(A):\n",
    "    \"\"\"Normalise the columns of a 2-D array or matrix to length one\n",
    "    A - array or matrix (which will be modified)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"A is not a 2-D array\")\n",
    "\n",
    "    number_of_columns = A.shape[1]\n",
    "    for i in range(number_of_columns):\n",
    "        A[:,i] /= np.linalg.norm(A[:,i], ord=2)\n",
    "\n",
    "\n",
    "def show_results(X_at_min, C):\n",
    "    \"\"\"Display the found arg min and compare with eigenvalues of C\n",
    "    X_at_min -- arguement at minimum found\n",
    "    C        -- symmetric matrix\n",
    "    \"\"\"\n",
    "    n,p = X_at_min.shape\n",
    "\n",
    "    normalize_columns(X_at_min)\n",
    "\n",
    "    # Get the eigenvectors belonging to the smallest eigenvalues\n",
    "    Eigen_Values, Eigen_Vectors = np.linalg.eig(C)\n",
    "    Permutation = Eigen_Values.argsort()\n",
    "    Smallest_Eigenvectors = Eigen_Vectors[:, Permutation[:p]]\n",
    "\n",
    "    if n < 10:\n",
    "        print(\"X_at_min               :\\n\", X_at_min)\n",
    "        print()\n",
    "        print(\"Smallest_Eigenvectors  :\\n\", Smallest_Eigenvectors)\n",
    "        print()\n",
    "    else:\n",
    "        Project_into_Eigenvectorspace = \\\n",
    "          Smallest_Eigenvectors * Smallest_Eigenvectors.T * X_at_min\n",
    "        Normal_Component = X_at_min - Project_into_Eigenvectorspace\n",
    "\n",
    "        print(\"norm(Normal_Component)/per entry :\", \\\n",
    "            np.linalg.norm(Normal_Component, ord=2) / float(n*p))\n",
    "\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_cg(initialise_proc):\n",
    "    \"\"\"Run minimisation with conjugate gradient algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_cg(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 fprime=gradient_for_vector,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 gtol=1.0000000000000001e-05,\n",
    "                                 norm=2,\n",
    "                                 epsilon=1.49011611938e-08,\n",
    "                                 maxiter=None,\n",
    "                                 full_output=0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_bfgs(initialise_proc):\n",
    "    \"\"\"Run minimisation with BFGS algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_bfgs(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 fprime=gradient_for_vector,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 gtol=1.0000000000000001e-05,\n",
    "                                 norm=2,\n",
    "                                 epsilon=1.49011611938e-08,\n",
    "                                 maxiter=None,\n",
    "                                 full_output=0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The minimum of the given cost function are matrices $ X $ which contain\n",
    "the $ p $ eigenvectors of $ C $ which are associated with the $ p $ smallest\n",
    "eigenvalues of $ C $. The order of the eigenvector in the minimum $ X $\n",
    "is defined by the order of the diagonal elements in $ N $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "=========  minimise_f_using_fmin(initialise_low_dimensional_data)  =========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 278\n",
      "         Function evaluations: 435\n",
      "X_at_min               :\n",
      " [[ 0.40268641 -0.08789988]\n",
      " [ 0.3776687  -0.89076053]\n",
      " [ 0.83379255  0.44589156]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[-0.40271356 -0.08791713]\n",
      " [-0.37766715 -0.89073914]\n",
      " [-0.83378013  0.4459309 ]]\n",
      "\n",
      "run_time : 0.10991200000000001\n",
      "============================================================================\n",
      "=======  minimise_f_using_fmin(initialise_higher_dimensional_data)  ========\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def initialise_low_dimensional_data():\n",
    "    \"\"\"Initialise the data, low dimensions\"\"\"\n",
    "    n = 3\n",
    "    p = 2\n",
    "    mu = 2.7\n",
    "\n",
    "    N = np.matrix(np.diag([2.5, 1.5]))\n",
    "    E = [1, 2, 3]\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def initialise_higher_dimensional_data():\n",
    "    \"\"\"Initialise the data, higher dimensions\"\"\"\n",
    "    n  = 20\n",
    "    p  =  5\n",
    "    mu = p + 0.5\n",
    "\n",
    "    N = np.matrix(np.diag(np.arange(p, 0, -1)))\n",
    "    E = np.arange(1, n+1)\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def run_and_time_all_tests():\n",
    "    \"\"\"Run all test and time them using a list of function names\"\"\"\n",
    "    List_of_Test_Names = [\"minimise_f_using_fmin\",\n",
    "                 \"minimise_f_using_fmin_powell\",\n",
    "                 \"minimise_f_using_fmin_cg\",\n",
    "                 \"minimise_f_using_fmin_bfgs\"]\n",
    "\n",
    "    List_of_Initialisations = [\"initialise_low_dimensional_data\",\n",
    "                               \"initialise_higher_dimensional_data\"]\n",
    "\n",
    "    for test_name in List_of_Test_Names:\n",
    "        for init_routine in List_of_Initialisations:\n",
    "            task_string  = test_name + \"(\" + init_routine + \")\"\n",
    "            line_length  = 76\n",
    "            spaces       = 2\n",
    "            left_padding = (line_length - len(task_string)) // 2\n",
    "            right_padding = line_length - left_padding - len(task_string)\n",
    "            print(\"=\" * line_length)\n",
    "            print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "                \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "            print(\"=\" * line_length)\n",
    "\n",
    "            start = time.clock()\n",
    "            exec(task_string)\n",
    "            run_time = time.clock() - start\n",
    "            print(\"run_time :\", run_time)\n",
    "\n",
    "run_and_time_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "norm(Normal_Component)/per entry : 0.00925386363206\n",
      "run_time : 3.090838\n",
      "============================================================================\n",
      "=====  minimise_f_using_fmin_powell(initialise_low_dimensional_data)  ======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 6\n",
      "         Function evaluations: 443\n",
      "X_at_min               :\n",
      " [[ 0.31171202  0.77545958]\n",
      " [ 0.66915412  0.23087818]\n",
      " [ 0.67458756 -0.58767142]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.31167604  0.77580995]\n",
      " [ 0.66929539  0.23058292]\n",
      " [ 0.67446403 -0.58732481]]\n",
      "\n",
      "run_time : 0.044489999999999696\n",
      "============================================================================\n",
      "====  minimise_f_using_fmin_powell(initialise_higher_dimensional_data)  ====\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 40.794384\n",
      "         Iterations: 19\n",
      "         Function evaluations: 20114\n",
      "norm(Normal_Component)/per entry : 0.000160442400483\n",
      "run_time : 1.7985209999999991\n",
      "============================================================================\n",
      "=======  minimise_f_using_fmin_cg(initialise_low_dimensional_data)  ========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 19\n",
      "         Function evaluations: 38\n",
      "         Gradient evaluations: 38\n",
      "X_at_min               :\n",
      " [[ 0.05748693 -0.15421743]\n",
      " [ 0.20875789 -0.96429861]\n",
      " [ 0.97627629  0.21527929]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.05748683 -0.15421662]\n",
      " [ 0.20875688 -0.96429933]\n",
      " [ 0.97627651  0.21527665]]\n",
      "\n",
      "run_time : 0.014658999999999978\n",
      "============================================================================\n",
      "======  minimise_f_using_fmin_cg(initialise_higher_dimensional_data)  ======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.727273\n",
      "         Iterations: 110\n",
      "         Function evaluations: 177\n",
      "         Gradient evaluations: 177\n",
      "norm(Normal_Component)/per entry : 4.56363591037e-09\n",
      "run_time : 0.0456310000000002\n",
      "============================================================================\n",
      "======  minimise_f_using_fmin_bfgs(initialise_low_dimensional_data)  =======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 13\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "X_at_min               :\n",
      " [[ 0.84188043 -0.37433445]\n",
      " [ 0.50003617  0.81201165]\n",
      " [ 0.20298072 -0.44778432]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.8418805   0.3743328 ]\n",
      " [ 0.50003598 -0.81201209]\n",
      " [ 0.20298089  0.4477849 ]]\n",
      "\n",
      "run_time : 0.01068800000000003\n",
      "============================================================================\n",
      "=====  minimise_f_using_fmin_bfgs(initialise_higher_dimensional_data)  =====\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.727273\n",
      "         Iterations: 132\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 142\n",
      "norm(Normal_Component)/per entry : 2.88510515061e-08\n",
      "run_time : 0.07790599999999959\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Get into groups of two or three and take turns explaining the following (about 2 minutes each):\n",
    "- regression vs classification\n",
    "- generative vs discriminative probabilistic methods\n",
    "- logistic regression\n",
    "- support vector machines\n",
    "- basis functions vs kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "This is the same dataset we used in Tutorial 3.\n",
    "\n",
    "*We will use an old dataset on the price of housing in Boston (see [description](https://www.kaggle.com/vikrishnan/boston-house-prices)). The aim is to predict the median value of the owner occupied homes from various other factors. We will use a normalised version of this data, where each row is an example. The median value of homes is given in the first column (the label) and the value of each subsequent feature has been normalised to be in the range $[-1,1]$. Download this dataset from [mldata.org](http://mldata.org/repository/data/download/csv/housing_scale/). The column names are ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
    "data = np.loadtxt('housing_scale.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing new kernels\n",
    "\n",
    "In the lectures, we saw that certain operations on kernels preserve positive semidefiniteness. Recall that a symmetric matrix $K\\in \\RR^n \\times\\RR^n$ is positive semidefinite if for all vectors $a\\in\\RR^n$ we have the inequality\n",
    "$$\n",
    "a^T K a \\geqslant 0.\n",
    "$$\n",
    "\n",
    "Prove the following relations:\n",
    "1. Given positive semidefinite matrices $K_1$, $K_2$, show that $K_1 + K_2$ is a valid kernel.\n",
    "2. Given a positive semidefinite matrix $K$, show that $K^2 = K\\cdot K$ is a valid kernel, where the multiplication is a pointwise multiplication (not matrix multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. We want to prove that\n",
    "$$a^T (K_1 + K_2) a \\geqslant 0.$$\n",
    "By linearity of addition, distribute the multiplication of $a$\n",
    "$$a^T (K_1 + K_2) a = a^T K_1 a^ + a^T K_2 a.$$\n",
    "By the definition of kernels, $a^T K_1 a \\geqslant 0$ and $a^T K_2 a \\geqslant 0$ for all $a$. \n",
    "Since the sum of two non-negative numbers is non-negative, we have shown that $K_1+K_2$ is a valid kernel.\n",
    "\n",
    "2. We may express $K$ in terms of its eigenvalues and eigenvectors as $K=\\sum\\limits_{i=1}^N\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T$\n",
    "\n",
    "where each $\\lambda_i$ and $\\mathbf{u}_i$ is an eigenvalue and eigenvector of $K$ (see equation 2.48 of Bishop).\n",
    "\n",
    "Proof:\n",
    "\n",
    "$K\\mathbf{u}_i = \\lambda_i \\mathbf{u}_i$ by the definition of eigenvalues and eigenvectors\n",
    "\n",
    "$K\\mathbf{u}_i\\mathbf{u}_i^T = \\lambda_i \\mathbf{u}_i\\mathbf{u}_i^T$ multiplying both sides by $\\mathbf{u}_i^T$\n",
    "\n",
    "$K\\sum\\limits_{i=1}^N \\mathbf{u}_i\\mathbf{u}_i^T=\\sum\\limits_{i=1}^N\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T$ sum over $N$ and move $K$ out of the summation\n",
    "\n",
    "$KUU^T=\\sum\\limits_{i=1}^N\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T$ where $U$ is a matrix whose columns are the eigenvectors of $K$\n",
    "\n",
    "$K=\\sum\\limits_{i=1}^N\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T$ because the columns form an orthonormal set, $U^TU=I$, $U^T=U^{-1}$ and so $UU^{-1}=UU^T=I$.\n",
    "\n",
    "Now we move to $K \\circ K$:\n",
    "\n",
    "$K \\circ K=\\sum\\limits_{i=1}^N\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T \\circ \\sum\\limits_{j=1}^N\\lambda_j\\mathbf{u}_j\\mathbf{u}_j^T$\n",
    "\n",
    "$=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N\\lambda_i\\lambda_j(\\mathbf{u}_i\\mathbf{u}_i^T) \\circ (\\mathbf{u}_j\\mathbf{u}_j^T)$\n",
    "\n",
    "$=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N\\lambda_i\\lambda_j(\\mathbf{u}_i \\circ \\mathbf{u}_j)(\\mathbf{u}_i \\circ \\mathbf{u}_j)^T$\n",
    "\n",
    "Each matrix $(\\mathbf{u}_i \\circ \\mathbf{u}_j)(\\mathbf{u}_i \\circ \\mathbf{u}_j)^T$ is positive semi-definite. This is because for any vectors $a$ and $v$, $a^Tvv^Ta=a^Tv(a^Tv)^T=(a^Tv)^2\\geq 0$. Because $K$ is positive semi-definite it has non-negative eigenvalues and so $\\lambda_i\\lambda_j\\geq0$ for all $i,j$, so multiplying by these scalars still returns a positive semi-definite matrix. By the identity shown in part 1, we know that sums of positive semi-definite matrices are also positive semi-definite.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Schur_product_theorem for this approach and other derivations of the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernel using closure\n",
    "\n",
    "Using the properties proven above, show that the inhomogenous polynomial kernel of degree 2\n",
    "$$k(\\mathbf{x},\\mathbf{x}') = (\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)^2$$\n",
    "is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Consider the dot product $\\dotprod{\\mathbf{x}}{\\mathbf{x}'}$ as the linear kernel.\n",
    "\\begin{align}\n",
    "k(\\mathbf{x},\\mathbf{x}') &= (\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)^2\\\\\n",
    "    &= (\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)(\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)\\\\\n",
    "    &= \\dotprod{\\mathbf{x}}{\\mathbf{x}'}^2 + 2\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1.\n",
    "\\end{align}\n",
    "\n",
    "This means that if we begin with a matrix $K'$ such that $K'_{ij} = \\dotprod{\\mathbf{x}}{\\mathbf{x}'}$, we can construct matrix $K$ such that $K_{ij}=k(\\mathbf{x},\\mathbf{x}') = K' \\circ K' + 2K' + \\mathbf{1}$. Here $\\mathbf{1}$ is the matrix of ones - it is possible to prove this is positive semi-definite by solving for its eigenvalues to show that they are non-negative, which is an equivalent condition. Since all of these matrices are positive semi-definite as shown above, their sum will also be positive semidefinite as we prevously showed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical comparison\n",
    "\n",
    "Recall from Tutorial 2 that we could explicitly construct the polynomial basis function. In fact this demonstrates the relation\n",
    "$$\n",
    "k(\\mathbf{x},\\mathbf{x}') = (\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)^2 = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{x}')}.\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = (x_1^2, \\ldots, x_n^2, \\sqrt{2}x_{i} x_j {\\forall i < j}, \\sqrt{2}x_1, \\ldots, \\sqrt{2}x_n, 1)\n",
    "$$\n",
    "*This is sometimes referred to as an explicit feature map or the primal version of a kernel method.*\n",
    "\n",
    "For the data above, construct two kernel matrices, one using the explicit feature map and the second using the equation for the polynomial kernel. Confirm that they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60.94556031  55.59204065  57.88864391  58.94056732  58.3315753 ]\n",
      " [ 55.59204065  62.90868854  62.38586554  62.6226549   63.19323354]\n",
      " [ 57.88864391  62.38586554  66.67591107  68.75468564  67.95803214]\n",
      " [ 58.94056732  62.6226549   68.75468564  76.50307387  74.51105349]\n",
      " [ 58.3315753   63.19323354  67.95803214  74.51105349  73.4119558 ]]\n",
      "[[ 60.94556031  55.59204065  57.88864391  58.94056732  58.3315753 ]\n",
      " [ 55.59204065  62.90868854  62.38586554  62.6226549   63.19323354]\n",
      " [ 57.88864391  62.38586554  66.67591107  68.75468564  67.95803214]\n",
      " [ 58.94056732  62.6226549   68.75468564  76.50307387  74.51105349]\n",
      " [ 58.3315753   63.19323354  67.95803214  74.51105349  73.4119558 ]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def phi_quadratic(x):\n",
    "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\"\"\"\n",
    "    D = len(x)\n",
    "    # Features are (1, {x_i}, {cross terms and squared terms})\n",
    "    # Cross terms x_i x_j and squared terms x_i^2 can be taken from upper triangle of outer product of x with itself\n",
    "    return np.hstack((1, np.sqrt(2)*x, np.sqrt(2)*np.outer(x, x)[np.triu_indices(D,k=1)], x ** 2))\n",
    "\n",
    "def feature_map(X):\n",
    "    \"\"\"Return the matrix of the feature map.\"\"\"\n",
    "    num_ex, num_feat = X.shape\n",
    "    Phi = np.zeros((num_ex, int(1+num_feat+num_feat*(num_feat+1)/2)))\n",
    "    for ix in range(num_ex):\n",
    "        Phi[ix,:] = phi_quadratic(X[ix,:])\n",
    "    return Phi\n",
    "\n",
    "def dotprod_quadratic(X):\n",
    "    \"\"\"Compute the kernel matrix using an explicit feature map of\n",
    "    the inhomogeneous polynomial kernel of degree 2\"\"\"\n",
    "    Phi = feature_map(X)\n",
    "    return np.dot(Phi, Phi.T)\n",
    "\n",
    "def kernel_quadratic(X,Y):\n",
    "    \"\"\"Compute the inhomogenous polynomial kernel matrix of degree 2\"\"\"\n",
    "    lin_dot = np.dot(X,Y.T)\n",
    "    dotprod = (lin_dot+1.)*(lin_dot + 1.)\n",
    "    return dotprod\n",
    "\n",
    "# Assume label is in the first column\n",
    "X = np.array(data[:, 1:])\n",
    "K = kernel_quadratic(X,X)\n",
    "Kfeat = dotprod_quadratic(X)\n",
    "\n",
    "print(K[:5,:5])\n",
    "print(Kfeat[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method of computing the kernel matrix is faster? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* computing $k(\\mathbf{x},\\mathbf{x}')=(\\dotprod{\\mathbf{x}}{\\mathbf{x}'} + 1)^2$ scales linearly with the length of $\\mathbf{x}$\n",
    "* computing $k(\\mathbf{x},\\mathbf{x}')=\\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{x}')}$ scales quadratically with the length of $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized least squares with kernels\n",
    "\n",
    "Show that the predictions using the regularized least squares solution can be expressed:\n",
    "$$\n",
    "y(\\mathbf{x}) = \\mathbf{k}(\\mathbf{x})^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{t}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{K}=\\mathbf{\\Phi}\\mathbf{\\Phi}^T$ and the vector $\\mathbf{k}(\\mathbf{x})$ contains elements $k_n(\\mathbf{x}) = k(\\mathbf{x}_n,\\mathbf{x})$.\n",
    "\n",
    "Describe the reason for each step in your working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Lecture 9. The important part is knowing the reason for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the weights for the regularised least squares solution are given by $\\mathbf{w} = \\left( \\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{t}$. \n",
    "\n",
    "By substituting $w = \\mathbf{\\Phi}^T \\mathbf{a}$, derive $\\mathbf{a}$ in terms of the kernel matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$\\mathbf{\\Phi}^T \\mathbf{a} = \\left( \\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{t}$\n",
    "\n",
    "$\\mathbf{a} = (\\mathbf{\\Phi}^T)^{-1} \\left( \\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{t}$\n",
    "\n",
    "$\\mathbf{a} = ( (\\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi})\\mathbf{\\Phi}^T)^{-1} \\mathbf{\\Phi}^T \\mathbf{t}$\n",
    "\n",
    "$\\mathbf{a} = ( (\\mathbf{\\Phi}^T)^{-1}(\\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi})\\mathbf{\\Phi}^T)^{-1} \\mathbf{t}$\n",
    "\n",
    "$\\mathbf{a} = ( \\lambda \\mathbf{I} + (\\mathbf{\\Phi}^T)^{-1}\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\mathbf{\\Phi}^T)^{-1} \\mathbf{t}$\n",
    "\n",
    "$\\mathbf{a} = ( \\lambda \\mathbf{I} + \\mathbf{K})^{-1} \\mathbf{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing solutions in $a$ and $\\mathbf{w}$\n",
    "\n",
    "Implement the kernelized regularized least squares as above. \n",
    "*This is often referred to as the dual version of regularized least squares.*\n",
    "\n",
    "Compare this with the solution from Tutorial 3. Implement two classes:\n",
    "* ```RLSPrimal```\n",
    "* ```RLSDual```\n",
    "\n",
    "each which contain a ```train``` and ```predict``` function.\n",
    "\n",
    "Think carefully about the interfaces to the training and test procedures for the two different versions of regularized least squares. Also think about the parameters that need to be stored in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: primal = 3.830002, dual = 3.830002\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "class RLSPrimal(object):\n",
    "    \"\"\"Primal Regularized Least Squares\"\"\"\n",
    "    def __init__(self, reg_param):\n",
    "        self.reg_param = reg_param\n",
    "        self.w = np.array([])   # This should be the number of features long\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Find the maximum likelihood parameters for the data X and labels y\"\"\"\n",
    "        Phi = feature_map(X)\n",
    "        num_ex = (Phi.T).shape[0]\n",
    "        A = np.dot(Phi.T, Phi) + self.reg_param * np.eye(num_ex)\n",
    "        b = np.dot(Phi.T, y)\n",
    "        self.w = np.linalg.solve(A, b)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Assume trained. Predict on data X.\"\"\"\n",
    "        Phi = feature_map(X)\n",
    "        return np.dot(Phi, self.w)\n",
    "\n",
    "class RLSDual(object):\n",
    "    def __init__(self, reg_param):\n",
    "        self.reg_param = reg_param\n",
    "        self.a = np.array([])    # This should be number of examples long\n",
    "\n",
    "    def train(self, K, y):\n",
    "        \"\"\"Find the maximum likelihood parameters for the kernel matrix K and labels y\"\"\"\n",
    "        num_ex = K.shape[0]\n",
    "        A = K + self.reg_param * np.eye(num_ex)\n",
    "        self.a = np.linalg.solve(A, y)\n",
    "    \n",
    "    def predict(self, K):\n",
    "        \"\"\"Assume trained. Predict on test kernel matrix K.\"\"\"\n",
    "        return np.dot(K, self.a)\n",
    "    \n",
    "    \n",
    "def split_data(data):\n",
    "    \"\"\"Randomly split data into two equal groups\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(N/2)]\n",
    "    test_idx = idx[int(N/2):]\n",
    "\n",
    "    # Assume label is in the first column\n",
    "    X_train = data[train_idx, 1:]\n",
    "    t_train = data[train_idx, 0]\n",
    "    X_test = data[test_idx, 1:]\n",
    "    t_test = data[test_idx, 0]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def rmse(label, prediction):\n",
    "    N = len(label)\n",
    "    return np.linalg.norm(label - prediction) / np.sqrt(N)\n",
    "\n",
    "X_train, t_train, X_test, t_test = split_data(data)\n",
    "P = RLSPrimal(1.1)\n",
    "P.train(X_train, t_train)\n",
    "pP = P.predict(X_test)\n",
    "\n",
    "K_train = kernel_quadratic(X_train, X_train)\n",
    "K_test = kernel_quadratic(X_test, X_train)   # This is not square\n",
    "D = RLSDual(1.1)\n",
    "D.train(K_train, t_train)\n",
    "pD = D.predict(K_test)\n",
    "print('RMSE: primal = %f, dual = %f' % (rmse(t_test, pP), rmse(t_test, pD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the pros and cons of using the primal or dual methods for regularized least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* The kernel approach is computationally independent of the number of features. If the data $X$ is of size $N \\times D$, then computing the kernel matrix $\\mathbf{\\Phi}\\mathbf{\\Phi}^T$ has cost $\\mathcal{O}(N^2D)$ independent of the features used.\n",
    "* The feature map approach has only a linear dependency on the number of examples. If the feature vector $\\phi$ is of dimension $M$, then computing the matrix $\\mathbf{\\Phi}^T\\mathbf{\\Phi}$ has cost $O(M^2N)$.\n",
    "* If $M$ is large relative to $N$ then the kernel approach is cheaper, otherwise the feature map approach is cheaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) General kernel\n",
    "\n",
    "Consider how you would generalise the two classes above if you wanted to have a polynomial kernel of degree 3. For the primal version, assume you have a function that returns the explicit feature map for the kernel ```feature_map(X)``` and for the dual version assume you have a function that returns the kernel matrix ```kernel_matrix(X)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
